{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNJ8DeRtAzaB"
   },
   "source": [
    "* <a href=\"https://colab.research.google.com/github/4dsolutions/clarusway_data_analysis/blob/main/DVwPY_S6/6-DVwPy_Review_Overview_Preview.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a><br/>\n",
    "* <a href=\"https://nbviewer.org/github/4dsolutions/clarusway_data_analysis/blob/main/DVwPY_S6/6-DVwPy_Review_Overview_Preview.ipynb\"><img align=\"left\" src=\"https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg\" alt=\"Open in nbviewer\" title=\"Open and View using nbviewer\"></a>\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "\n",
    "## <p style=\"background-color:#FDFEFE; font-family:newtimeroman; color:#9d4f8c; font-size:100%; text-align:center; border-radius:10px 10px;\">WAY TO REINVENT YOURSELF</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYlfU09rAzaI"
   },
   "source": [
    "## <p style=\"background-color:#9d4f8c; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">REVIEW, OVERVIEW, PREVIEW</p>\n",
    "\n",
    "<a data-flickr-embed=\"true\" href=\"https://www.flickr.com/photos/kirbyurner/52563704012/in/album-72177720296706479/\" title=\"LMS Dashboard\"><img src=\"https://live.staticflickr.com/65535/52563704012_71ef4beb8a_b.jpg\" width=\"1024\" height=\"354\" alt=\"LMS Dashboard\"></a><script async src=\"//embedr.flickr.com/assets/client-code.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "Welcome to the conclusion of Data Visualization, with Python.\n",
    "\n",
    "The Lesson Plan for today is to recap all we learned from Seaborn, and even matplotlib, using an entirely different library:  plotly.\n",
    "\n",
    "Let's cut away to that segment, and return here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYlfU09rAzaI"
   },
   "source": [
    "The skillsets we have been most developing in the foreground encompass:\n",
    "\n",
    "* Jupyter -- how we keep our Notes and sometimes publish them\n",
    "* numpy -- the number crunching king of vectorized operations\n",
    "* matplotlib -- [pyplot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html) imitates Matlab's way of doing things\n",
    "* pandas -- sophisticated DataFrames, isomorphic to spreadsheets*\n",
    "* seaborn -- augmented data visualization, beyond anything up to now\n",
    "* plotly -- our final sessions dive into this visualization alternative\n",
    "* python -- Python itself has more power, as one learns to tap it\n",
    "* Regexes -- pattern matching, a standalone mini-language incorporated by Python\n",
    "* LaTeX -- we use it to share math typography in a Jupyter Notebook context\n",
    "\n",
    "A core theme of both Data Analysis with Python, and this course, has been Exploratory Data Analysis (EDA).  We imagine the data is a black box at first, and over time we learn to get a sense of it.  Even though we share the details of visualizations in Part 2 (this part), they apply throughout Part 1 i.e. you may start using the power of visualizations right away in EDA.\n",
    "\n",
    "Then come more skillsets we might optionally cultivate in the background.  Plotly takes us into Flash.  Running Notebooks online (in the cloud) connects us to Docker.\n",
    "\n",
    "Throughout this course, we have alluded to SQL, which accomplished many of the very same tasks but through a different semantics.  We use them both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "## <p style=\"background-color:#9d4f8c; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">CONTENT</p>\n",
    "\n",
    "Topics We Covered:\n",
    "\n",
    "* numpy - vectorized powerhouse\n",
    "* pandas - series + tabular dataframes + plots\n",
    "* matplotlib - plots (2D and 3D)\n",
    "* seaborn - fancier plots\n",
    "* plotly - interactive online plots (dash)\n",
    "\n",
    "Topics potentially \"Ahead\":\n",
    "\n",
    "* ml  (machine learning)\n",
    "    - sklearn\n",
    "    - tensorflow\n",
    "    - pytorch\n",
    "    - other\n",
    "* sql  (structured query language)\n",
    "* python (there's always a next level)\n",
    "* geospatial data (in the cards for us all -- plotly does some of this)\n",
    "* [jupyter books](https://jupyterbook.org/en/stable/intro.html) (extending notebook skills)\n",
    "\n",
    "We say \"ahead\" because you may have come from there.  We suggest more stats, but maybe you are coming from knowing stats.\n",
    "\n",
    "In Preview mode then, lets talk about:  *Machine Learning*.\n",
    "\n",
    "Since the very first slides, we have emphasized that what we're picturing is appying data analysis (including visualization) skills as a way of preparing and including some kind of model making.  A mental model may be enough, if the data fits some expected and understood pattern (\"retail sales\"), but we thrive on computational algorithms, and this is where ML comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5TMW6XaAzaJ"
   },
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "## <p style=\"background-color:#9d4f8c; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">Preview:  What is ML?</p>\n",
    "Throughout the subject of Statistics, a core aim has been prediction.  Since the dawn of history, we have had a need to divine the future, yet encounter limits on predictability.  \n",
    "\n",
    "The age old project to distill our intuitions about such concepts as \"likelihood\", \"expectations\", \"confidance\", into a science is part of the heritage of data science.\n",
    "\n",
    "With what measure of confidance do we expect X to happen?  Does my measure of confidance change with respect to some \"by when?\".  I might expect X to happen someday with 100% confidance, yet with no confidance at all about tomorrow or next week.  \n",
    "\n",
    "These may sound like obvious truths, however in getting clear on such topics, we invent new methods of computation.  We don't stop with some hazy notion of \"average\"; we break it out into mean, medium and mode.\n",
    "\n",
    "We hope to intelligently anticipate (more than wildly or blindly guess), based on the many models we may have developed.  \n",
    "\n",
    "By model, we could mean a simulation.  Does our simulation run as a computer program?  Not necessarily.  People were simulating and internalizing complex systems long before they had invented silicon chip computers.  In some cases, they simply kept careful records.\n",
    "\n",
    "We could also mean, my \"model\", some actual Python object developed for us by one of the model makers (KM, SVM...).\n",
    "\n",
    "Who are the model makers in this context?  Are those people?\n",
    "\n",
    "The model makers are well-known and/or still experimental algorithms.  We categorize them in various ways, such as into supervised and unsupervised. \n",
    "\n",
    "A good example of a working model would be a recommendation engine attached to a website.  \"Based on your choice of YouTubes so far, a next one of interest might be...\".  No human being needed to study your viewing history on order for the machine to make a recommendation.\n",
    "\n",
    "A model need not be mysterious and/or opaque, but it could be. \n",
    "\n",
    "A simple line through a bunch of dots well summarizes many of them.  \n",
    "\n",
    "However some models attain their high powers of prediction at the expense of being able to give us an explicit set of rules.\n",
    "\n",
    "[A Quick List of ML Algorithms](https://howtolearnmachinelearning.com/articles/a-quick-list-of-machine-learning-algorithms/)\n",
    "\n",
    "More concretely, in the supervised learning setting, we show the features (X) and the right answers (y) to a \"learner\" or \"recognizer\" known as a neural net, consider deep after N layers.  The feedback of getting it wrong or right is used to \"weight\" the \"neurons\" through a process known as \"gradient descent\" (uses calculus, partial derivatives, finds a downward path for an error function).\n",
    "\n",
    "The above paragraph puts into words a lot of number crunchy math, which numpy is good at.  What we get back from such a process is a Python object that has powers of prediction.  But to what degree?  How good is it?  Should we turn it loose in the real world?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mN0CH7VfAzaK"
   },
   "source": [
    "## <p style=\"background-color:#9d4f8c; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">THE ML PROCESS</p>\n",
    "\n",
    "Think about how you yourself learn from experience?  Having a strong sense of a right answer or how you want things to go, is motivational.  Machine Learning sets up a similar feedback loop in the algebra, especially in a supervised setting.\n",
    "\n",
    "The main question before us is one of reliability.  Does our model do the job?  Is a newer model an improvement?\n",
    "\n",
    "One standard approach, to test reliability, is as follows:\n",
    "\n",
    "* give the right answers on a percentage of the total data (training)\n",
    "* try the model against the balance (the rest), never seen\n",
    "* do more testing\n",
    "\n",
    "As you explore Sci-Kit Learn, you will find this standard approach is baked in, through time-saving functions that automatically divvy the data into training and testing sets.\n",
    "\n",
    "The process is akin to shuffling cards as the same total data may be divvyed into testing and training in multiple ways, allowing for more averaging and perhaps fine tuning.  We looked at similar bootstrapping ideas in connection with the Confidance Interval (Seaborn barplot etc.).\n",
    "\n",
    "What's left out of the above description is the whole matter of selecting the appropriate ML engine (model maker), with fine tuned hyperparameters. \n",
    "\n",
    "That process itself suggests a feedback loop:  why not try several ML engines on the same data and find by experiment which seems to work best. Indeed, that's a thing.\n",
    "\n",
    "You will find sklearn support \"ensembles\" of model makers to work together. Sometimes they each reach a conclusion then hold a vote.  Random Forests made of Decision Trees behave in ensemble fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9eNE2ugAzaK"
   },
   "source": [
    "## <p style=\"background-color:#9d4f8c; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">Cluster Finding Machines</p>\n",
    "\n",
    "Finding clusters is sometimes an \"unsupervised\" form of ML, meaning we do not have a dataset with \"right answers\" for training purposes.  We are looking for patterns in target data without knowing ourselves what they are beforehand.  \n",
    "\n",
    "Other times, we have our clusters defined for training data, in which case the process is \"supervised\".\n",
    "\n",
    "For example, we might want to cluster tweets and short reports into \"topics\" (what the cluster around).  We don't know in advance what those topics will be.  Here's a tutorial \n",
    "\n",
    "When learning about cluster finding, we also learn about cluster making.  Here's a data science teacher mixing some clustered data, in order to discover if SVM (one of the model makers) will find them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citing the tutorial at [General Abstract Nonsense](https://generalabstractnonsense.com/2017/03/A-quick-look-at-Support-Vector-Machines/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# create some V shaped data\n",
    "np.random.seed(6)\n",
    "# normalized floats, 200 rows, 2 columns\n",
    "X = np.random.randn(200, 2)\n",
    "X[:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rule here is:  y is True if the right column exceeds the absolute value of the left.  Throw away negative signs on the left and ask if the resulting number is less than what's to its right.  y is False if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a \"predict me\" vector\n",
    "y = X[:, 1] > np.absolute(X[:, 0])\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recast y as 1s and negative 1s using the ever-useful `np.where`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.where(y, 1, -1)\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided data, we find the Support Vector Machine is pretty good at teasing apart the y=1 from the y=-1, but will not get it right 100% of the time.\n",
    "\n",
    "We're not requiring `mlxtend.plotting` to run this Notebook, but Google colab has it, so here's a screen shot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a data-flickr-embed=\"true\" href=\"https://www.flickr.com/photos/kirbyurner/52313045333/in/dateposted-public/\" title=\"Screen Shot 2022-08-26 at 11.25.15 AM\"><img src=\"https://live.staticflickr.com/65535/52313045333_a499f458dc_z.jpg\" width=\"640\" height=\"513\" alt=\"Screen Shot 2022-08-26 at 11.25.15 AM\"></a><script async src=\"//embedr.flickr.com/assets/client-code.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a Support Vector Classifier using the rbf kernel\n",
    "svm = SVC(kernel='rbf', random_state=0, gamma=0.5, C=10.0)\n",
    "svm.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make up some new X points.  Point = any number of features (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[-0.99, 1.1],\n",
    "                  [.12, .33],\n",
    "                  [-3, 1]])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would the model predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would we consider correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y = data[:, 1] > np.absolute(data[:, 0])\n",
    "new_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do more study into the reliability of svm in this instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y, svm.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y == svm.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSlEdW1A20Io"
   },
   "source": [
    "## <p style=\"background-color:#9d4f8c; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">Categorizing Machines</p>\n",
    "\n",
    "**Clickbait Versus Headlines**\n",
    "\n",
    "We will need pandas for this next example, with its amazing ability to read csv files over the web, taking a URL as input.\n",
    "\n",
    "The file below is actually just a txt file, a csv with no headers, and it's delimited by tab.  No problemo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GL3nsXNYAzaL"
   },
   "outputs": [],
   "source": [
    "df_clickbait = pd.read_csv(\"https://raw.githubusercontent.com/sixhobbits/sklearn-intro/master/clickbait.txt\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clickbait.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clickbait.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clickbait.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clickbait.columns = [\"Headline\", \"Category\"]\n",
    "df_clickbait.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "headlines = df_clickbait[\"Headline\"]\n",
    "labels = df_clickbait[\"Category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break dataset into test and train sets\n",
    "train_headlines = headlines[:8000]\n",
    "test_headlines = headlines[8000:]\n",
    "\n",
    "train_labels = labels[:8000]\n",
    "test_labels = labels[8000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember one-hot encoding, otherwise know as [get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) in pandas?  We were able to unpack a column, full of ungainly strings, into new columns, with 1s and 0s for values.\n",
    "\n",
    "That's close to what we do when \"vectorizing\" a \"bag of words\".  The specific algorithm we will be using, already in the can for us in sklearn, throws away what we might call \"stop words\" (used too frequently) and also words that occur too infrequently.\n",
    "\n",
    "We might do some data cleaning of front, before we vectorize.  Remove dates?  Excise serial numbers?  The end goal is to distill a document space to a vocabulary and frequency count for each word.  The ML engine (which one we pick) should be able to munch on this kind of numeric data.\n",
    "\n",
    "[TF-IDF in Python with Scikit Learn](https://youtu.be/i74DVqMsRWY) -- From Python Tutorials for the Digital Humanities\n",
    "\n",
    "[Topic Modeling](http://topic-modeling.pythonhumanities.com/01_01_introduction_to_topic_modeling.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to initializing a vectorizer, which is used in a next cell, we have to make the important decision regarding which ML engine to use.  In this case:  LinearSVC.\n",
    "\n",
    "LinearSVC is a subtype of a Support Vector Machine (SVM).  When separating data into clusters, you want to find a \"cut\" or \"slice\" through the data that maximizes its distance from the closest element in any cluster.  Optimizing means making the cut ever more effective at delineating two groups.\n",
    "\n",
    "In this case, we are attempting to separate the data using known cluster scores (1 or 0) for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "svm = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our text data into numerical vectors\n",
    "train_vectors = vectorizer.fit_transform(train_headlines)\n",
    "test_vectors = vectorizer.transform(test_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier and predict on test set\n",
    "svm.fit(train_vectors, train_labels)\n",
    "\n",
    "predictions = svm.predict(test_vectors)\n",
    "\n",
    "accuracy_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_headlines = [\"18 Must-Have Shoes For Fall\", \n",
    "                 'Weather to Improve this Coming Summer']\n",
    "new_vectors = vectorizer.transform(new_headlines)\n",
    "new_predictions = svm.predict(new_vectors)\n",
    "\n",
    "new_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eppisf6o20Ip"
   },
   "source": [
    "### <p style=\"background-color:#9d4f8c; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">Review:  Linear Regression</p>\n",
    "\n",
    "We should probably do one with `tips` first, as that's the one we've squeezed the most out of in DV0-DV5 (Data Visualization with Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "BtAf43Va20Iq",
    "outputId": "5e76994f-847a-4efd-c3db-8c2bb994a7d5"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(kind=\"reg\", data=tips, x=\"total_bill\", y=\"tip\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls roller*.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're reviewing here is sqlite3, which we only touched upon briefly, in connection with `airports.db` -- a flatfile of some 7K world airports, categorized in a few ways, some with lat/long.  That's a thumbnail data dictionary.\n",
    "\n",
    "Here's a link to it [on Kaggle](https://www.kaggle.com/datasets/jonatancr/airports).\n",
    "\n",
    "A potentially interesting project would be to compare the Roller Coasters dataset on Kaggle, with the one provided.  Another idea for a capstone project.  Perhaps a newer table could be merged from both sources?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sql.connect(\"roller_coasters.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coasters_df = pd.read_sql(\"select * from Coasters;\", con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coasters_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Linear Regression model shall we try?  The \"vertical drop\" and the \"speed\" would seem connected by the exceptionless law of gravity, whatever that is.  Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(kind=\"reg\", data=coasters_df, x=\"VertDrop\", y=\"Speed\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems like a pretty good fit if you ask me.  Call this the simplest of the Machine Learning model makers, the \"LRM\" or \"linear regression machine\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now let's pause to exercise our secondary skills, which soon could become primary.  Let's modify our SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coasters_df = pd.read_sql(\"select name, park, state, country from Coasters ORDER BY country, state, name;\", con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coasters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMowtQqH20Ir"
   },
   "source": [
    "### <p style=\"background-color:#9d4f8c; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">Overview:  Python's Context Manager Construct</p>\n",
    "\n",
    "We went down this rabbit hole at least once.  Or call it \"following a trail\" (they all branch into each other, in a large dark forest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CM:\n",
    "    \n",
    "    def __init__(self, myname):\n",
    "        self.name = myname\n",
    "        \n",
    "    def __enter__(self):\n",
    "        print(\"with me('') as it:  do something with it\")\n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, *oops):\n",
    "        print(\"done doing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CM(\"talker\") as it:\n",
    "    print(\"I'm inside the context -- __enter__ has run\")\n",
    "    print(it.name)\n",
    "    print(\"I will exit now...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets reuse that same basic skeleton to talk to databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "\n",
    "class CM:\n",
    "    \n",
    "    def __init__(self, myname):\n",
    "        self.name = myname\n",
    "        \n",
    "    def __enter__(self):\n",
    "        try:\n",
    "            self.conn = sql.connect(self.name)\n",
    "        except:\n",
    "            print(\"No connection\")\n",
    "            raise\n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, *oops):  # trap error data if any\n",
    "        self.conn.close()\n",
    "        if oops[0]:       # hoping oops is (None, None, None)\n",
    "            return False  # something exceptional happened\n",
    "        return True       # all OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CM(\"roller_coasters.db\") as db:\n",
    "    curs = db.conn.cursor()\n",
    "    curs.execute(\"select * from Coasters ORDER By name;\")\n",
    "    for rec in curs.fetchall():\n",
    "        print(rec[0])\n",
    "    print(\"--- GN\")\n",
    "print(\"--- Connection closed\")\n",
    "db.conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"background-color:#9d4f8c; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">END OF DVwPY 6<br/>(Preview, Overview, Review)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flznMCqEAzac"
   },
   "source": [
    "___\n",
    "\n",
    "\n",
    "## <p style=\"background-color:#FDFEFE; font-family:newtimeroman; color:#9d4f8c; font-size:100%; text-align:center; border-radius:10px 10px;\">WAY TO REINVENT YOURSELF</p>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DV_capstone_solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
